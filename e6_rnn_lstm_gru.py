# -*- coding: utf-8 -*-
"""E6-RNN_LSTM_GRU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eItvBAeqExxUziZGRoDJI2y58_sQ57Cq

# Exercise 6

## Predict rating using LSTM
"""

import pandas as pd

#Se extrae una base de entrenamiento de un csv en una ruta de github
dataTraining = pd.read_csv('https://github.com/sergiomora03/AdvancedTopicsAnalytics/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)

plots = dataTraining['plot']
# Se crea una variable dummy donde true indica que la calificación es mayor o igual que la media y los valores False indican que la calificación es menor que la media.
y = (dataTraining['rating'] >= dataTraining['rating'].mean()).astype(int)

plots

y

"""# Exercise 6.1

- Remove stopwords
- Lowercase
- split the text in words
- pad_sequences
"""

!pip install wget
!pip install livelossplot --quiet

import pandas as pd
import numpy as np
import wget
import os
from zipfile import ZipFile

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string

#Se importan diversas librerías y clases necesarias para construir y entrenar modelos de redes neuronales en Keras, así como para realizar visualizaciones y trabajar con modelos de procesamiento de lenguaje natural (NLP) utilizando gensim

from sklearn.model_selection import train_test_split

from keras import backend as K
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.utils import pad_sequences
from livelossplot import PlotLossesKeras

import matplotlib.pyplot as plt

import gensim
from gensim.models import Word2Vec
import warnings


warnings.filterwarnings('ignore')
nltk.download('stopwords')
nltk.download('punkt')

X = plots

"""Se realiza el preprocesamiento de los datos como: Convertir el texto a minúsculas utilizando text.lower(). Eliminar los signos de puntuación del texto utilizando una comprensión de lista y la función string.punctuation. Tokenizar el texto en palabras utilizando word_tokenize del módulo NLTK. Eliminar stopwords en ingles Unir las palabras procesadas en una cadena de texto utilizando ' '.join(tokens) y devolverla como resultado."""

stop_words = set(stopwords.words('english'))
def preprocess(text):
    text = text.lower()
    text = ''.join([word for word in text if word not in string.punctuation])
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

X = X.apply(preprocess)

X = X.tolist()

# construye un vocabulario a partir de los caracteres presentes en la trama
voc = set(''.join(X))#connjunto de caracteres únicos a partir de la cadena de caracteres resultante.
vocabulary = {x: idx + 1 for idx, x in enumerate(set(voc))}#crea un diccionario de comprensión que asigna a cada carácter único en el conjunto un índice único más uno.

#conversión de caracteres a números y el relleno (padding) de secuencias.
# Max len
max_len = 200 #Máximo deseado para las secuencias de texto.
X = [x[:max_len] for x in X] # Asegura que cada texto en la lista X tenga como máximo una longitud de 200 caracteres.
# Convert characters to int and pad
X = [[vocabulary[x1] for x1 in x if x1 in vocabulary.keys()] for x in X]#: Este código convierte cada carácter en los textos de X en un número utilizando un vocabulario predefinido (vocabulary).

X_pad = pad_sequences(X, maxlen=max_len)#aplica el relleno (padding) a las secuencias de entrada

"""# Exercise 6.2

Create a SimpleRNN neural network to predict the rating of a movie

Calculate the testing set accuracy
"""

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Suponiendo que tienes X y y preprocesados y listos
# X_pad es el resultado del padding que ya has aplicado
# y es la variable binaria a predecir

# Convertir y en formato numpy
y = np.array(y)

# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)

# Definir la arquitectura del modelo
model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario
model.add(SimpleRNN(64))  # Número de unidades en la capa SimpleRNN que permite al modelo aprender y capturar dependencias temporales en los datos de entrada, como secuencias de texto o series temporales.
model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide

# Compilar el modelo: Se mide el modelo de acuerdo a la minimización de perdida con la cosentropia binaria y maximizando el accuracy:
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Calcular la precisión en el conjunto de pruebas
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Testing Set Accuracy:", test_accuracy)

"""Se llega a un accuracy del 51,4%, indicando que el modelo sale igual a predecir aleatoriamente las categorias

# Exercise 6.3

Create a LSTM neural network to predict the rating of a movie

Calculate the testing set accuracy
"""

from tensorflow.keras.layers import LSTM, Dense, Embedding
# Definir la arquitectura del modelo
model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario
model.add(LSTM(64))  # Número de unidades en la capa LSTM que permite al modelo aprender y capturar dependencias a largo plazo en los datos de entrada, como secuencias de texto o series temporales
model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide

# Compilar el modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Calcular la precisión en el conjunto de pruebas
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Testing Set Accuracy:", test_accuracy)

"""Este modelo secuencial genera una mejor medida de accuracy con respecto al modelo simple RNN

# Exercise 6.4

Create a GRU neural network to predict the rating of a movie

Calculate the testing set accuracy
"""

from tensorflow.keras.layers import GRU, Dense, Embedding

# Definir la arquitectura del modelo
model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario
model.add(GRU(64))  # Número de unidades en la capa GRU (Gated Recurrent Unit), lo que permite al modelo aprender y capturar dependencias en los datos de entrada, como secuencias de texto o series temporales.
model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide

# Compilar el modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Calcular la precisión en el conjunto de pruebas
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Testing Set Accuracy:", test_accuracy)

"""Con la implementación de una capa GRU se evidencia que es mejor que el RNN simple pero es inferior su rendimiento con respecto al modelo con capa LSTM"""