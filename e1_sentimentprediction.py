# -*- coding: utf-8 -*-
"""E1-SentimentPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wRsd3yX1OZ1RDQmpRZ1jMuS134FQsg_y

# Exercise 1

## Analyze how travelers expressed their feelings on Twitter

A sentiment analysis job about the problems of each major U.S. airline.
Twitter data was scraped from February of 2015 and contributors were
asked to first classify positive, negative, and neutral tweets, followed
by categorizing negative reasons (such as "late flight" or "rude service").
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# %matplotlib inline
import matplotlib.pyplot as plt

# Se lee un archivo de csv de una ruta de github
tweets = pd.read_csv('https://github.com/sergiomora03/AdvancedTopicsAnalytics/raw/main/datasets/Tweets.zip', index_col=0)

tweets.head()

tweets.shape

"""### Proportion of tweets with each sentiment"""

tweets['airline_sentiment'].value_counts()

"""Se observa que los tweets de las aerolineas en su mayoria se representan por la clasificación negativa por parte de los usuarios

### Proportion of tweets per airline
"""

tweets['airline'].value_counts()

"""Se observa que la aerolinea United es la que mas se menciona en los tweets, lo contrario pasa con la aerolina Virgin America"""

pd.Series(tweets["airline"]).value_counts().plot(kind = "bar",figsize=(8,6),rot = 0)

pd.crosstab(index = tweets["airline"],columns = tweets["airline_sentiment"]).plot(kind='bar',figsize=(10, 6),alpha=0.5,rot=0,stacked=True,title="Sentiment by airline")

"""De la gráfica anterior se evidencia que Southwest es la aerolinea con mayores tweets positivos. Sin embargo, la aerolinea Virgin America es la que tiene una mayor proporción de tweets positivos si se compara contra ella misma (muy balanceado con las otras categorias).
Por otra parte, United es la aerolina con mayores tweets negativos

# Exercise 1.1

Predict the sentiment using CountVectorizer

use Random Forest classifier
"""

#Se importan librerias para entrenar modelos, vectorizar texto, construir bosquea aleatorios y estemizar
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer

#Los textos de los tweets son la variable explicativa, mientras que la clasificación del sentimiento hacia la aerolinea es la variable dependiente
X = tweets['text']
y = tweets['airline_sentiment'].map({'negative':-1,'neutral':0,'positive':1})

#Vectorización
vect = CountVectorizer()
# Se convierte el texto en una matriz de palabras y representa la frecuencia de cada palabra en cada documento
X_dtm = vect.fit_transform(X)

X_dtm

X_dtm.todense()

temp=X_dtm.todense()
temp

vect.vocabulary_

#Hay 14640 documentos con 15051 caracteristicas
X_dtm.shape

print(list(vect.vocabulary_.keys())[-150:-100])

# Se convierte la matriz dispersa X_dtm en densa, luego se accede al primer documento en la colección de documentos representados para encontrar el índice del término que tiene la frecuencia más alta en ese primer documento.
X_dtm.todense()[0].argmax()

#RandomForest: Se aplica el metodo de clasificación de bosque aleatorio y se hace una validación cruzada partiendo la base en 5 partes
rf=RandomForestClassifier()
pd.Series(cross_val_score(rf, X_dtm, y, cv=5)).describe()

# Se define una función que acepta un vectorizador y calcula el accuracy
def tokenize_test(vect):
    X_dtm = vect.fit_transform(X)
    print('Features: ', X_dtm.shape[1])
    rf = RandomForestClassifier()
    print(pd.Series(cross_val_score(rf, X_dtm, y, cv=5)).describe())

print(pd.Series(cross_val_score(rf, X_dtm, y, cv=5)).describe())

"""Este modelo tiene un rendimiento para clasificar el sentimiento de los tweets en un 72,8%

# Exercise 1.2

Se remueven stopwords, luego se predice el sentimiento usando CountVectorizer.

use Random Forest classifier
"""

def tokenize_test(vect):
    X_dtm = vect.fit_transform(X)
    print('Features: ', X_dtm.shape[1])

    rf = RandomForestClassifier()
    scores = cross_val_score(rf, X_dtm, y, cv=5)
    print(pd.Series(scores).describe())


vect = CountVectorizer(stop_words='english')
tokenize_test(vect)

"""Usando el vectorizador disminuye un 3% de rendimiento en la clasificación del modelo

# Exercise 1.3

Increase n_grams size (with and without stopwords),  then predict the sentiment using CountVectorizer

use Random Forest classifier
"""

def tokenize_test(vect):
    X_dtm = vect.fit_transform(X)
    print('Features: ', X_dtm.shape[1])

    rf = RandomForestClassifier()
    scores = cross_val_score(rf, X_dtm, y, cv=5)
    print(pd.Series(scores).describe())

vect = CountVectorizer(ngram_range=(1, 2), max_features=1000) #Se vectoriza tomando ngramas de 1 a 2 palabras y se maximiza a 1000 caracteristicas
tokenize_test(vect)

"""Sigue siendo un mejor modelo el random forest sin vectorización ya que con esta forma de vectorizar es 2% menos efectivo en predecir las clases de sentimientos"""

def tokenize_test(vect):
    X_dtm = vect.fit_transform(X)
    print('Features: ', X_dtm.shape[1])

    rf = RandomForestClassifier()
    scores = cross_val_score(rf, X_dtm, y, cv=5)
    print(pd.Series(scores).describe())

vect = CountVectorizer(stop_words='english')
vect = CountVectorizer(ngram_range=(1, 2), max_features=1000)
tokenize_test(vect)

print(vect)

"""# Exercise 1.4

Predict the sentiment using TfidfVectorizer.

use Random Forest classifier
"""

# Vectorización utilizando TfidfVectorizer
vect = TfidfVectorizer()
X_dtm = vect.fit_transform(X)

# Definir una función que acepta un vectorizador y calcula la precisión
def tokenize_test(vect):
    X_dtm = vect.fit_transform(X)
    print('Features: ', X_dtm.shape[1])
    rf = RandomForestClassifier()

    # Imprimir estadísticas de validación cruzada
    print(pd.Series(cross_val_score(rf, X_dtm, y, cv=5)).describe())

# Llamar a la función con TfidfVectorizer
tokenize_test(vect)

"""El random forest inicial sigue siendo mejor modelo ya que mantiene un AUC superior"""

vect2= TfidfVectorizer(stop_words='english')

# Llamar a la función con TfidfVectorizer
tokenize_test(vect2)

"""Integrantes:

*   Rosemary Ríos Pulido
*   John Sebastian Martinez Cipagauta
*   Carlos Felipe Mora Garzón
*   Ian Nicolas Rincon Tavera
*   Andres Parra

"""