# -*- coding: utf-8 -*-
"""E5-NeuralNetworksKeras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTU7AlzupEI6hWLvvXXAh_dQGftaFaAU

# Excercise 5
## Neural Networks in Keras

Use keras framework to solve the below exercises.
"""

#Se realiza importación de librerias
import numpy as np
import keras
import pandas as pd
import matplotlib.pyplot as plt

"""## 5.1 Predict rating of a movie using Keras

**Exercise:** Use keras framework to predict rating.
"""

#Se importa la información de un archivo csv en cierta ruta especifica
dataTraining = pd.read_csv('https://github.com/sergiomora03/AdvancedTopicsAnalytics/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)

dataTraining.head()

"""La tabla contiene registros de titulos de peliculas con su descripción, genero, puntuación y año de estreno"""

plots = dataTraining['plot']
# Se crea una variable dummy donde true indica que la calificación es mayor o igual que la media y los valores False indican que la calificación es menor que la media.
y = (dataTraining['rating'] >= dataTraining['rating'].mean()).astype(int)

plots

y

"""## Data Precosessing

- Remove stopwords
- Lowercase
- split the text in words
- pad_sequences
"""

!pip install wget
!pip install livelossplot --quiet

import pandas as pd
import numpy as np
import wget
import os
from zipfile import ZipFile

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import string

#Se importan diversas librerías y clases necesarias para construir y entrenar modelos de redes neuronales en Keras, así como para realizar visualizaciones y trabajar con modelos de procesamiento de lenguaje natural (NLP) utilizando gensim
from sklearn.model_selection import train_test_split

from keras import backend as K
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.utils import pad_sequences
from livelossplot import PlotLossesKeras

import matplotlib.pyplot as plt

import gensim
from gensim.models import Word2Vec
import warnings


warnings.filterwarnings('ignore')
nltk.download('stopwords')
nltk.download('punkt')

X = plots

"""Se realiza el preprocesamiento de los datos como:
Convertir el texto a minúsculas utilizando text.lower().
Eliminar los signos de puntuación del texto utilizando una comprensión de lista y la función string.punctuation.
Tokenizar el texto en palabras utilizando word_tokenize del módulo NLTK.
Eliminar stopwords en ingles
Unir las palabras procesadas en una cadena de texto utilizando ' '.join(tokens) y devolverla como resultado.
"""

stop_words = set(stopwords.words('english'))
def preprocess(text):
    text = text.lower()
    text = ''.join([word for word in text if word not in string.punctuation])
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

X = X.apply(preprocess)

X.head()

X = X.tolist()

# construye un vocabulario a partir de los caracteres presentes en la trama
voc = set(''.join(X))#connjunto de caracteres únicos a partir de la cadena de caracteres resultante.
vocabulary = {x: idx + 1 for idx, x in enumerate(set(voc))}#crea un diccionario de comprensión que asigna a cada carácter único en el conjunto un índice único más uno.

vocabulary

len(vocabulary)

#conversión de caracteres a números y el relleno (padding) de secuencias.
# Max len
max_len = 200 #Máximo deseado para las secuencias de texto.
X = [x[:max_len] for x in X] # segura que cada texto en la lista X tenga como máximo una longitud de 150 caracteres.
# Convert characters to int and pad
X = [[vocabulary[x1] for x1 in x if x1 in vocabulary.keys()] for x in X]#: Este código convierte cada carácter en los textos de X en un número utilizando un vocabulario predefinido (vocabulary).

len(X)

X_pad = pad_sequences(X, maxlen=max_len)#aplica el relleno (padding) a las secuencias de entrada

X_pad

# Se hace una partición de 30% de datos en prueba y el restante en entrenamiento, ademas se aplica un parametro de stratify para asegurar una evaluación más justa del modelo cuando las clases en la variable objetivo están desbalanceadas.
X_train, X_test, y_train, y_test = train_test_split(X_pad, y, stratify = y, test_size = 0.3, random_state = 18)

"""## Build Model


Se construye un modelo de red neuronal para la clasificación binaria utilizando una capa de embedding seguida de una capa densa con activación sigmoide. El modelo se configura para minimizar la pérdida binaria durante el entrenamiento utilizando el optimizador SGD, y se evalúa el rendimiento del modelo utilizando la métrica de exactitud (accuracy)
"""

learning_rate = 0.01

model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])

model.summary()

import keras.optimizers as opts #importa los optimizadores de Keras. Pa

K.clear_session()#limpia la sesión de Keras para asegurarse de que no haya modelos anteriores o capas definidas en la sesión.


K.clear_session()

model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len))
model.add(Dense(64, activation='relu'))  # Primera capa intermedia con 64 unidades y activación ReLU
model.add(Dense(32, activation='relu'))  # Segunda capa intermedia con 32 unidades y activación ReLU
model.add(Dense(1, activation='sigmoid'))

op = opts.SGD(lr=learning_rate)

model.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy'])

model.summary()

model.fit(X_train, y_train, validation_data=[X_test, y_test],
          batch_size=128, epochs=20, verbose=1,
          callbacks=[PlotLossesKeras()])#ntrena el modelo. Se especifican los datos de entrenamiento y visualizar la pérdida y la precisión durante el entrenamiento).

"""      Se observa que al realizar el nuevo modelo con mas capas y con funciones de activación relu con su parametrización de minimizar la perdida con base a tener el mejor AUC, se evidencia que el modelo de red neuronal predice bien y no tiene riesgos de overfiting ya que el entrenamiento y la validación tuvieron el mismo rendimiento de predicción y la misma perdida"""

K.clear_session()#limpia la sesión de Keras para asegurarse de que no haya modelos anteriores o capas definidas en la sesión.

model = Sequential()
model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len))
model.add(Dense(1, activation='sigmoid'))#Agrega una capa densa con una neurona de salida y activación sigmoide.

op = opts.SGD(lr=learning_rate)#optimizador SGD con la tasa de aprendizaje especificada.

model.compile(loss='binary_crossentropy', optimizer=op, metrics=['accuracy']) # Compila el modelo. Esto configura el proceso de entrenamiento del modelo. Se especifica la función de pérdida (binary_crossentropy )

model.summary()

model.fit(X_train, y_train, validation_data=[X_test, y_test],
          batch_size=128, epochs=20, verbose=1,
          callbacks=[PlotLossesKeras()])#Se entrena el modelo. Se especifican los datos de entrenamiento y visualizar la pérdida y la precisión durante el entrenamiento).

"""En este modelo de red neuronal realizado con una capa y una función de activación sigmoide refleja que el accuracy aumentó pero se ve que se pierde mas en el modelo a medida que aprende mas de los datos, dando a entender posible riesgo de overfiting

# 5.2 Decision Boundary --  Moons Dataset

**Exercise:** Use keras framework to find a decision boundary for point in the make_moons.
"""

# Create moons dataset.

from sklearn.datasets import make_moons

x_train, y_train = make_moons(n_samples=1000, noise= 0.2, random_state=3)
plt.figure(figsize=(12, 8))
plt.scatter(x_train[:, 0], x_train[:,1], c=y_train, s=40, cmap=plt.cm.Spectral);

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# Definir el modelo de la red neuronal
model = Sequential()
model.add(Dense(64, input_dim=2, activation='relu')) #Se añade una capa densa con 64 neuronas, una entrada de dimensión 2 y función de activación ReLU
model.add(Dense(32, activation='relu')) #Se añade una capa densa con 32 neuronas y función de activación ReLU
model.add(Dense(1, activation='sigmoid')) #Se añade una capa densa con 1 neurona y función de activación sigmoide

# Compilar el modelo: Se establece que el rendimiento del modelo se medirá con la exactitud, por medio de una tasa de aprendizaje del 1% utilizando la función de pérdida 'binary_crossentropy'
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])

# Entrenar el modelo
history = model.fit(x_train, y_train, epochs=50, batch_size=32, verbose=1)

"""El modelo entrenado de acuerdo a la función de perdida parametrizado, genera un accuracy del 97,9% de efectividad en la predicción

**Hint:** Use the next function to plt the decision boundary,
"""

# Función para visualizar la frontera de decisión
def plot_decision_region(model, X, pred_fun):
    min_x = np.min(X[:, 0])
    max_x = np.max(X[:, 0])
    min_y = np.min(X[:, 1])
    max_y = np.max(X[:, 1])
    min_x = min_x - (max_x - min_x) * 0.05
    max_x = max_x + (max_x - min_x) * 0.05
    min_y = min_y - (max_y - min_y) * 0.05
    max_y = max_y + (max_y - min_y) * 0.05
    x_vals = np.linspace(min_x, max_x, 30)
    y_vals = np.linspace(min_y, max_y, 30)
    XX, YY = np.meshgrid(x_vals, y_vals)
    grid_r, grid_c = XX.shape
    ZZ = np.zeros((grid_r, grid_c))
    for i in range(grid_r):
        for j in range(grid_c):
            ZZ[i, j] = pred_fun(model, XX[i, j], YY[i, j])
    plt.contourf(XX, YY, ZZ, 30, cmap=plt.cm.coolwarm, vmin=0, vmax=1)
    plt.colorbar()
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title("Decision Boundary")

def pred_fun(model, x1, x2):
    xval = np.array([[x1, x2]])
    return (model.predict(xval) > 0.5).astype("int32")[0][0]

plt.figure(figsize=(8, 6))
plot_decision_region(model, x_train, pred_fun)
plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.Spectral, s=40)
plt.show()

"""En el grafico se puede observar como se separan las dos clases de peliculas:
Las que tienen una puntuación por encima del promedio y las que no en donde se observa una buena separación de las dos clases
"""