# -*- coding: utf-8 -*-
"""E8-QuestionAnswer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UogbgVkXwgp_jHMN1PziR95E8GKL33S9

## Question & Answer

Creating a Question-Answer Transformer model or QA Transformer can be beneficial for several reasons, particularly in the field of Natural Language Processing (NLP). Here are some compelling reasons why you might want to develop a QA Transformer:

1. **Question-Answering Systems:** QA Transformers are designed to provide accurate and contextually relevant answers to questions posed in natural language. These systems have a wide range of practical applications, including chatbots, virtual assistants, customer support, and information retrieval.

2. **Information Retrieval:** QA Transformers can be used to search through large corpora of text and extract precise answers to user queries. This can improve the efficiency and effectiveness of information retrieval systems.

3. **Document Summarization:** QA Transformers can be used to summarize long documents by answering questions about the document's content. This makes it easier for users to quickly understand the key points and relevant information in a text.

4. **Education and E-Learning:** QA Transformers can be integrated into educational platforms to provide instant answers and explanations to students' questions. They can also help with the automatic generation of quiz questions and answers.

5. **Content Generation:** QA Transformers can assist in content generation by automatically answering questions based on available knowledge. This can be useful for generating FAQs, product descriptions, and informative articles.

6. **Customer Support:** Many companies use QA systems to automate responses to frequently asked questions, freeing up human agents to handle more complex queries and providing customers with quick solutions.

7. **Medical Diagnosis:** QA Transformers can assist medical professionals by answering questions related to patient records, medical literature, and diagnostic information, potentially leading to faster and more accurate diagnoses.

8. **Legal and Compliance:** In the legal field, QA Transformers can be used to search and extract information from legal documents, assisting lawyers in their research and case preparation.

9. **Language Translation:** QA Transformers can be used to answer questions about language translation, helping users understand the meaning of words, phrases, or sentences in different languages.

10. **Scientific Research:** QA Transformers can support researchers by answering questions related to scientific literature, allowing them to quickly access relevant information for their studies.

11. **Decision Support:** QA Transformers can aid in decision-making processes by providing answers to questions related to data analysis, market research, and business intelligence.

12. **Accessibility:** QA Transformers can improve accessibility for individuals with disabilities by providing spoken or written answers to their questions, helping them access information more easily.

Overall, QA Transformers have the potential to enhance information retrieval, automation, and user interaction in various domains, making them a valuable tool in the development of intelligent systems and applications. The ability to provide accurate and context-aware answers to questions in natural language is a key advantage of these models.

---
Exercise:

Now, as a data scientist expert in NLP, you are asked to create a model to be able to answer question in Spanish. Your stakeholders will pass you an article and one question and your model should answer it.
"""

# Se intala una biblioteca para extraer datos de archivos HTML y XML
!pip install requests beautifulsoup4
# Se instala un paquete para realizar traducciones de texto entre diferentes idiomas utilizando la API de Google Translate.
!pip install googletrans==4.0.0-rc1

import requests
from bs4 import BeautifulSoup
# Se realiza web scraping para obtener el contenido de un artículo de la página web especificada, utilizando solicitudes HTTP para obtener la página y BeautifulSoup para analizar y extraer el texto del artículo
# URL del artículo
url = "https://time.com/collection/time100-ai/6309026/geoffrey-hinton/"

# Realizar una solicitud HTTP para obtener el contenido de la página
response = requests.get(url)

# Verificar si la solicitud fue exitosa
if response.status_code == 200:
    # Analizar el contenido HTML de la página con BeautifulSoup
    soup = BeautifulSoup(response.text, "html.parser")

    # Encuentra el contenido del artículo en la página utilizando las etiquetas y clases HTML adecuadas
    article_content = soup.find("div", {"class": "article-content"})

    # Extraer el texto del artículo
    article_text = ""
    for paragraph in article_content.find_all("p"):
        article_text += paragraph.get_text() + "\n"

    # Imprimir el texto del artículo
    print(article_text)
else:
    print("Error al obtener la página:", response.status_code)

question = "How is Geoffrey Hinton?"

from transformers import pipeline
from googletrans import Translator

#Se carga un modelo preentrenado de BERT finetuned para responder preguntas en varios idiomas (multi-cased) utilizando el modelo y el tokenizador especificados, y crea un pipeline listo para responder preguntas basado en este modelo.

nlp = pipeline("question-answering", model="mrm8488/bert-multi-cased-finetuned-xquadv1", tokenizer="mrm8488/bert-multi-cased-finetuned-xquadv1")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

#Otra variante del Modelo de BERT

#No es precisa la respuesta

question = "How is Geoffrey Hinton?"

#Modelo nlp bert
nlp = pipeline("question-answering", model="bert-large-uncased-whole-word-masking-finetuned-squad", tokenizer="bert-large-uncased-whole-word-masking-finetuned-squad")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

#Otra variante del Modelo de BERT con otra pregunta

question = "What is Geoffrey Hinton's concern?"

#Modelo nlp bert
nlp = pipeline("question-answering", model="bert-large-uncased-whole-word-masking-finetuned-squad", tokenizer="bert-large-uncased-whole-word-masking-finetuned-squad")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

#Modelo de ALBERT (A Lite BERT)
# Se carga un modelo preentrenado de ALBERT finetuned en el conjunto de datos SQuAD 2.0, junto con su tokenizador asociado, y crea un pipeline listo para responder preguntas basado en este modelo

#Es mas pesada la ejecución aunque es mas acertado

question = "How is Geoffrey Hinton?"

#Modelo nlp bert
nlp = pipeline("question-answering", model="mfeb/albert-xxlarge-v2-squad2", tokenizer="mfeb/albert-xxlarge-v2-squad2")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

#Modelo de Roberta  (A Robustly Optimized BERT Pretraining Approach)
#Se carga un modelo preentrenado de RoBERTa (versión base) finetuned en el conjunto de datos SQuAD 2.0, junto con su tokenizador asociado, y crea un pipeline listo para responder preguntas basado en este modelo
#Es el mejor resultado, rapida ejecución y más acertado

question = "How is Geoffrey Hinton?"

#Modelo nlp bert
nlp = pipeline("question-answering", model="deepset/roberta-base-squad2", tokenizer="deepset/roberta-base-squad2")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

#Modelo de Roberta

#Otra pregunta

question = "What is Geoffrey Hinton's concern?"

#Modelo nlp bert
nlp = pipeline("question-answering", model="deepset/roberta-base-squad2", tokenizer="deepset/roberta-base-squad2")

#Respuesta modelo
answer = nlp(question=question, context=article_text)

#Traducir respuesta al español con googletrans
translator = Translator()
translated_answer = translator.translate(answer["answer"], src="en", dest="es")

#Respuesta final
print("Respuesta del modelo:")
print(answer)
print("Respuesta traducida:")
print(translated_answer)

"""En resumen, BERT es el modelo original que sentó las bases para muchos desarrollos posteriores en modelos de lenguaje basados en Transformers. RoBERTa es una versión optimizada y más robusta de BERT, mientras que ALBERT se enfoca en mejorar la eficiencia y escalabilidad del modelo original."""