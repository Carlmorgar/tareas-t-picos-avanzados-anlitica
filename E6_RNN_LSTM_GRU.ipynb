{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUgBjMmm0YAx"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "## Predict rating using LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRNZB4ktu2mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkKZ4Kf0YAz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vppi3Q6C0YA0"
      },
      "outputs": [],
      "source": [
        "#Se extrae una base de entrenamiento de un csv en una ruta de github\n",
        "dataTraining = pd.read_csv('https://github.com/sergiomora03/AdvancedTopicsAnalytics/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEfDJEWx0YA1"
      },
      "outputs": [],
      "source": [
        "plots = dataTraining['plot']\n",
        "# Se crea una variable dummy donde true indica que la calificación es mayor o igual que la media y los valores False indican que la calificación es menor que la media.\n",
        "y = (dataTraining['rating'] >= dataTraining['rating'].mean()).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HQ8i4Hq0YA1",
        "outputId": "8cb5023e-4b4c-40b4-96de-7436f3fd3775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3107    most is the story of a single father who takes...\n",
              "900     a serial killer decides to teach the secrets o...\n",
              "6724    in sweden ,  a female blackmailer with a disfi...\n",
              "4704    in a friday afternoon in new york ,  the presi...\n",
              "2582    in los angeles ,  the editor of a publishing h...\n",
              "                              ...                        \n",
              "8417    \" our marriage ,  their wedding .  \"  it ' s l...\n",
              "1592    the wandering barbarian ,  conan ,  alongside ...\n",
              "1723    like a tale spun by scheherazade ,  kismet fol...\n",
              "7605    mrs .  brisby ,  a widowed mouse ,  lives in a...\n",
              "215     tinker bell journey far north of never land to...\n",
              "Name: plot, Length: 7895, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9duKIhOA0YA2",
        "outputId": "5750baed-c205-4d3e-ac0b-3963ef169782"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3107    1\n",
              "900     0\n",
              "6724    1\n",
              "4704    1\n",
              "2582    1\n",
              "       ..\n",
              "8417    0\n",
              "1592    0\n",
              "1723    0\n",
              "7605    1\n",
              "215     1\n",
              "Name: rating, Length: 7895, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmsQHxM0YA2"
      },
      "source": [
        "# Exercise 6.1\n",
        "\n",
        "- Remove stopwords\n",
        "- Lowercase\n",
        "- split the text in words\n",
        "- pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nIVx7kg0YA3",
        "outputId": "230df38a-f029-4636-b416-0ccca459cc07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=a01c4561678084acd51519c4b59a62af6d3eddae283be9cac4a32906ac8bdcc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install livelossplot --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wget\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "#Se importan diversas librerías y clases necesarias para construir y entrenar modelos de redes neuronales en Keras, así como para realizar visualizaciones y trabajar con modelos de procesamiento de lenguaje natural (NLP) utilizando gensim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.utils import pad_sequences\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "EmLe_mP5oKTU",
        "outputId": "7b362cf0-6192-446b-e65f-105da1270134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = plots"
      ],
      "metadata": {
        "id": "x9Fe_tEdoPkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza el preprocesamiento de los datos como: Convertir el texto a minúsculas utilizando text.lower(). Eliminar los signos de puntuación del texto utilizando una comprensión de lista y la función string.punctuation. Tokenizar el texto en palabras utilizando word_tokenize del módulo NLTK. Eliminar stopwords en ingles Unir las palabras procesadas en una cadena de texto utilizando ' '.join(tokens) y devolverla como resultado."
      ],
      "metadata": {
        "id": "9588CbpECNw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([word for word in text if word not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X = X.apply(preprocess)"
      ],
      "metadata": {
        "id": "F7v-oYgdp56G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.tolist()\n"
      ],
      "metadata": {
        "id": "KbDPKkPWpLkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construye un vocabulario a partir de los caracteres presentes en la trama\n",
        "voc = set(''.join(X))#connjunto de caracteres únicos a partir de la cadena de caracteres resultante.\n",
        "vocabulary = {x: idx + 1 for idx, x in enumerate(set(voc))}#crea un diccionario de comprensión que asigna a cada carácter único en el conjunto un índice único más uno."
      ],
      "metadata": {
        "id": "gZwYjiVfppNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conversión de caracteres a números y el relleno (padding) de secuencias.\n",
        "# Max len\n",
        "max_len = 200 #Máximo deseado para las secuencias de texto.\n",
        "X = [x[:max_len] for x in X] # Asegura que cada texto en la lista X tenga como máximo una longitud de 200 caracteres.\n",
        "# Convert characters to int and pad\n",
        "X = [[vocabulary[x1] for x1 in x if x1 in vocabulary.keys()] for x in X]#: Este código convierte cada carácter en los textos de X en un número utilizando un vocabulario predefinido (vocabulary).\n"
      ],
      "metadata": {
        "id": "3x6rVPGfpbxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pad = pad_sequences(X, maxlen=max_len)#aplica el relleno (padding) a las secuencias de entrada"
      ],
      "metadata": {
        "id": "jwzcNAZJpgWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6.2\n",
        "\n",
        "Create a SimpleRNN neural network to predict the rating of a movie\n",
        "\n",
        "Calculate the testing set accuracy"
      ],
      "metadata": {
        "id": "DQ14SQaqR8N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Suponiendo que tienes X y y preprocesados y listos\n",
        "# X_pad es el resultado del padding que ya has aplicado\n",
        "# y es la variable binaria a predecir\n",
        "\n",
        "# Convertir y en formato numpy\n",
        "y = np.array(y)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir la arquitectura del modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario\n",
        "model.add(SimpleRNN(64))  # Número de unidades en la capa SimpleRNN que permite al modelo aprender y capturar dependencias temporales en los datos de entrada, como secuencias de texto o series temporales.\n",
        "model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide\n",
        "\n",
        "# Compilar el modelo: Se mide el modelo de acuerdo a la minimización de perdida con la cosentropia binaria y maximizando el accuracy:\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Calcular la precisión en el conjunto de pruebas\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing Set Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "iL4pewpBR7nn",
        "outputId": "ab5866c5-2be6-4dda-b4ec-d980a7edb6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "178/178 [==============================] - 17s 83ms/step - loss: 0.6995 - accuracy: 0.4998 - val_loss: 0.7003 - val_accuracy: 0.4763\n",
            "Epoch 2/10\n",
            "178/178 [==============================] - 13s 72ms/step - loss: 0.6912 - accuracy: 0.5387 - val_loss: 0.7067 - val_accuracy: 0.4921\n",
            "Epoch 3/10\n",
            "178/178 [==============================] - 12s 70ms/step - loss: 0.6918 - accuracy: 0.5318 - val_loss: 0.7021 - val_accuracy: 0.4921\n",
            "Epoch 4/10\n",
            "178/178 [==============================] - 13s 71ms/step - loss: 0.6926 - accuracy: 0.5234 - val_loss: 0.7021 - val_accuracy: 0.5063\n",
            "Epoch 5/10\n",
            "178/178 [==============================] - 13s 70ms/step - loss: 0.6900 - accuracy: 0.5368 - val_loss: 0.7020 - val_accuracy: 0.4794\n",
            "Epoch 6/10\n",
            "178/178 [==============================] - 13s 71ms/step - loss: 0.6882 - accuracy: 0.5449 - val_loss: 0.7062 - val_accuracy: 0.5095\n",
            "Epoch 7/10\n",
            "178/178 [==============================] - 13s 73ms/step - loss: 0.6875 - accuracy: 0.5523 - val_loss: 0.7047 - val_accuracy: 0.4921\n",
            "Epoch 8/10\n",
            "178/178 [==============================] - 16s 89ms/step - loss: 0.6874 - accuracy: 0.5387 - val_loss: 0.7072 - val_accuracy: 0.4937\n",
            "Epoch 9/10\n",
            "178/178 [==============================] - 14s 77ms/step - loss: 0.6867 - accuracy: 0.5477 - val_loss: 0.7108 - val_accuracy: 0.4953\n",
            "Epoch 10/10\n",
            "178/178 [==============================] - 13s 74ms/step - loss: 0.6868 - accuracy: 0.5463 - val_loss: 0.7113 - val_accuracy: 0.4937\n",
            "50/50 [==============================] - 1s 18ms/step - loss: 0.6956 - accuracy: 0.5149\n",
            "Testing Set Accuracy: 0.5148828625679016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se llega a un accuracy del 51,4%, indicando que el modelo sale igual a predecir aleatoriamente las categorias"
      ],
      "metadata": {
        "id": "SmjSRPsNEJKV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki8anSMk0YA3"
      },
      "source": [
        "# Exercise 6.3\n",
        "\n",
        "Create a LSTM neural network to predict the rating of a movie\n",
        "\n",
        "Calculate the testing set accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfaGPYvw0YA3",
        "outputId": "e9834c1f-72e8-453c-e923-4340840bab33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "178/178 [==============================] - 33s 168ms/step - loss: 0.6930 - accuracy: 0.5127 - val_loss: 0.6933 - val_accuracy: 0.5301\n",
            "Epoch 2/10\n",
            "178/178 [==============================] - 28s 158ms/step - loss: 0.6905 - accuracy: 0.5267 - val_loss: 0.6970 - val_accuracy: 0.4652\n",
            "Epoch 3/10\n",
            "178/178 [==============================] - 27s 155ms/step - loss: 0.6890 - accuracy: 0.5303 - val_loss: 0.6974 - val_accuracy: 0.4858\n",
            "Epoch 4/10\n",
            "178/178 [==============================] - 29s 162ms/step - loss: 0.6884 - accuracy: 0.5452 - val_loss: 0.6983 - val_accuracy: 0.4984\n",
            "Epoch 5/10\n",
            "178/178 [==============================] - 28s 156ms/step - loss: 0.6872 - accuracy: 0.5466 - val_loss: 0.6981 - val_accuracy: 0.4826\n",
            "Epoch 6/10\n",
            "178/178 [==============================] - 28s 155ms/step - loss: 0.6858 - accuracy: 0.5533 - val_loss: 0.7055 - val_accuracy: 0.4810\n",
            "Epoch 7/10\n",
            "178/178 [==============================] - 28s 155ms/step - loss: 0.6843 - accuracy: 0.5563 - val_loss: 0.7003 - val_accuracy: 0.4763\n",
            "Epoch 8/10\n",
            "178/178 [==============================] - 27s 150ms/step - loss: 0.6836 - accuracy: 0.5559 - val_loss: 0.7038 - val_accuracy: 0.4826\n",
            "Epoch 9/10\n",
            "178/178 [==============================] - 29s 161ms/step - loss: 0.6805 - accuracy: 0.5621 - val_loss: 0.7088 - val_accuracy: 0.4794\n",
            "Epoch 10/10\n",
            "178/178 [==============================] - 29s 164ms/step - loss: 0.6799 - accuracy: 0.5695 - val_loss: 0.7120 - val_accuracy: 0.4573\n",
            "50/50 [==============================] - 2s 36ms/step - loss: 0.6989 - accuracy: 0.5218\n",
            "Testing Set Accuracy: 0.5218492746353149\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "# Definir la arquitectura del modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario\n",
        "model.add(LSTM(64))  # Número de unidades en la capa LSTM que permite al modelo aprender y capturar dependencias a largo plazo en los datos de entrada, como secuencias de texto o series temporales\n",
        "model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Calcular la precisión en el conjunto de pruebas\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing Set Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo secuencial genera una mejor medida de accuracy con respecto al modelo simple RNN"
      ],
      "metadata": {
        "id": "ukOZ348YFZlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6.4\n",
        "\n",
        "Create a GRU neural network to predict the rating of a movie\n",
        "\n",
        "Calculate the testing set accuracy"
      ],
      "metadata": {
        "id": "3V3dPxvORw6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "\n",
        "# Definir la arquitectura del modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary) + 1, 128, input_length=max_len)) #Capa embedding para procesar secuencias de texto con una dimensión de el largo del vocabulario mas 1, una dimensión del vector de 128 palabras y una longitud máxima de las secuencias de entrada por el largo del vocaboluario\n",
        "model.add(GRU(64))  # Número de unidades en la capa GRU (Gated Recurrent Unit), lo que permite al modelo aprender y capturar dependencias en los datos de entrada, como secuencias de texto o series temporales.\n",
        "model.add(Dense(1, activation='sigmoid')) #Función de activación sigmoide\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Calcular la precisión en el conjunto de pruebas\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing Set Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "MLjL9dMtRwVJ",
        "outputId": "91457499-ee7c-4af2-a788-2ce2bcb6bec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "178/178 [==============================] - 28s 141ms/step - loss: 0.6927 - accuracy: 0.5179 - val_loss: 0.6938 - val_accuracy: 0.5316\n",
            "Epoch 2/10\n",
            "178/178 [==============================] - 24s 136ms/step - loss: 0.6909 - accuracy: 0.5262 - val_loss: 0.6967 - val_accuracy: 0.5095\n",
            "Epoch 3/10\n",
            "178/178 [==============================] - 25s 138ms/step - loss: 0.6900 - accuracy: 0.5311 - val_loss: 0.6982 - val_accuracy: 0.4968\n",
            "Epoch 4/10\n",
            "178/178 [==============================] - 26s 146ms/step - loss: 0.6890 - accuracy: 0.5384 - val_loss: 0.6981 - val_accuracy: 0.5111\n",
            "Epoch 5/10\n",
            "178/178 [==============================] - 26s 147ms/step - loss: 0.6877 - accuracy: 0.5475 - val_loss: 0.7000 - val_accuracy: 0.4826\n",
            "Epoch 6/10\n",
            "178/178 [==============================] - 25s 139ms/step - loss: 0.6873 - accuracy: 0.5417 - val_loss: 0.6982 - val_accuracy: 0.5269\n",
            "Epoch 7/10\n",
            "178/178 [==============================] - 25s 140ms/step - loss: 0.6861 - accuracy: 0.5489 - val_loss: 0.7024 - val_accuracy: 0.4984\n",
            "Epoch 8/10\n",
            "178/178 [==============================] - 25s 139ms/step - loss: 0.6848 - accuracy: 0.5545 - val_loss: 0.7034 - val_accuracy: 0.4905\n",
            "Epoch 9/10\n",
            "178/178 [==============================] - 25s 139ms/step - loss: 0.6830 - accuracy: 0.5575 - val_loss: 0.7033 - val_accuracy: 0.5063\n",
            "Epoch 10/10\n",
            "178/178 [==============================] - 27s 150ms/step - loss: 0.6807 - accuracy: 0.5614 - val_loss: 0.7041 - val_accuracy: 0.4778\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 0.6979 - accuracy: 0.5168\n",
            "Testing Set Accuracy: 0.5167827606201172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la implementación de una capa GRU se evidencia que es mejor que el RNN simple pero es inferior su rendimiento con respecto al modelo con capa LSTM"
      ],
      "metadata": {
        "id": "lqBCUt3vGL34"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}